# Superstore ‚Äî SQL & Data Analysis

Short summary
--------------
This repository contains a practical, recruiter-friendly analysis of the "Sample ‚Äî Superstore" dataset.
Work here is SQL-first: the data is cleaned and the key analysis is implemented as a set of ordered SQL scripts. The repo also contains a short executive summary and dashboard notes suitable for a portfolio or interview.

Author: Abhinav Verma ‚Äî https://github.com/Abhinav-TheAnalyst / https://www.linkedin.com/in/abhinav-theanalyst/

What you'll find here
----------------------
- `sql/` ‚Äî main analysis (run `01_data_cleaning.sql` ‚Üí `07_final_insights.sql`).
- `reports/executive_summary.md` ‚Äî one-page findings and recommended next steps.
- `dashboard/` ‚Äî notes and example screenshots for quick visuals (Power BI / Tableau).
- `notebooks/` ‚Äî optional notebook for local exploration (kept for convenience).

Notes about the data
--------------------
- The raw dataset is not included. To reproduce results, download the Sample ‚Äî Superstore CSV from Kaggle and place it locally at `data/raw/Sample - Superstore.csv`.
- A cleaned CSV placeholder (`data/processed/superstore_cleaned.csv`) is present so reviewers can inspect column names and a few sample rows without the raw file.

Quick start (minimal)
---------------------
1) Place the original dataset locally (do not commit it):

   - `data/raw/Sample - Superstore.csv` (Kaggle link in `reports/executive_summary.md`).

2) (Optional) If you want to run the cleaner or notebook, create a small virtual environment and install requirements from `archive_removed` where helper files are preserved:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r archive_removed\requirements.txt
```

3) Run the cleaner (if you restored the script from `archive_removed`):

```powershell
python archive_removed\scripts\clean_data.py --input "data/raw/Sample - Superstore.csv" --output "data/processed/superstore_cleaned.csv"
```

4) Load `data/processed/superstore_cleaned.csv` into your database (Postgres, SQLite, DuckDB) and run the SQL scripts in `sql/` in order.

Notes for reviewers
-------------------
- The heart of the project is in the SQL scripts. For a quick review, read `reports/executive_summary.md` and then open `sql/02_sales_overview.sql` and `sql/05_discount_impact.sql`.
- I did not commit raw data; if you want ready-made PNGs or a SQLite dump, I can generate them locally and add them to `dashboard/screenshots/` on request.

If you'd like a shorter one-page README for hiring managers or a longer technical appendix, tell me which version and I will prepare it.
````markdown
# üè¨ Superstore SQL Business Analysis

## üìå Project Overview

This project performs **end-to-end SQL-driven business analysis** on the *Sample ‚Äì Superstore* dataset. The goal is to identify **profitability drivers, loss-making areas, and actionable business insights** that management can use to improve performance.

This is a **recruiter-focused project** showcasing SQL skills, business thinking, and structured analysis.

---

## üéØ Business Problem

Retail company wants to understand:

* Which **regions, states, and categories** generate profit
* Which **products and discounts** cause losses
* Who are the **most valuable customers**
* How **discounting impacts profitability**

---

## üìä Dataset Description

**Dataset:** Sample ‚Äì Superstore (Kaggle)
# Superstore ‚Äî SQL & Data Analysis

## Overview

This repository contains a practical, intermediate-level analysis of the "Sample ‚Äî Superstore" dataset. The goal is to show end-to-end work: clean the data, load it into a simple analysis-ready format, run SQL-driven analysis, and produce concise business insights and recommendations. The project is written in a clear, human tone so it reads naturally for recruiters and hiring managers.

Source data comes from Kaggle: https://www.kaggle.com/datasets/vivek468/superstore-dataset-final

Author: Abhinav Verma ‚Äî https://github.com/Abhinav-TheAnalyst / https://www.linkedin.com/in/abhinav-theanalyst/

---

## What you'll find here

- `data/raw/Superstore.csv` ‚Äî original dataset (not committed here)
- `data/processed/superstore_cleaned.csv` ‚Äî cleaned, analysis-ready CSV (generated by the script)
- `scripts/clean_data.py` ‚Äî lightweight Python script (pandas) to clean and export the dataset
- `sql/` ‚Äî collection of SQL queries used for the analysis, organized as step-by-step scripts
- `sql/table_schema.sql` ‚Äî suggested table schema for loading into a relational DB
- `reports/executive_summary.md` ‚Äî short, recruiter-friendly summary and recommendations
- `dashboard/README.md` ‚Äî quick notes on building a dashboard (Power BI / Tableau)
- `requirements.txt` ‚Äî minimal Python deps
- `.gitignore` ‚Äî sensible ignores for this project

---

## Quick start (PowerShell)

1) Create a Python environment and install requirements:

```powershell
python -m venv .venv; .\\.venv\\Scripts\\Activate.ps1
pip install -r requirements.txt
```

2) Place the original `Superstore.csv` in `data/raw/`.

3) Run the cleaning script to produce `data/processed/superstore_cleaned.csv`:

```powershell
python scripts\\clean_data.py --input "data/raw/Superstore.csv" --output "data/processed/superstore_cleaned.csv"
```

4) Load the cleaned CSV into your RDBMS of choice (Postgres/MySQL/SQLite) or run the SQL files directly against a local SQLite DB after importing the CSV.

---

## Recommended workflow

1. Clean and standardize data using `scripts/clean_data.py`.
2. Load the cleaned CSV into a database (or use pandas / DuckDB for analysis).
3. Run the SQL scripts in `sql/` in order (01 ‚Üí 07) to reproduce the analysis.
4. Use `reports/executive_summary.md` for the top-level talking points for interviews or the hiring manager.
 
````
